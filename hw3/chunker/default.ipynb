{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunker: baseline program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the baseline solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:05<00:00, 202.11it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('data', 'train.txt.gz'), os.path.join('data', 'chunker'), '.tar')\n",
    "decoder_output = chunker.decode('data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the baseline output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 12035 phrases; correct: 9216.\n",
      "accuracy:  86.66%; (non-O)\n",
      "accuracy:  87.74%; precision:  76.58%; recall:  77.47%; FB1:  77.02\n",
      "             ADJP: precision:  50.00%; recall:  19.03%; FB1:  27.56  86\n",
      "             ADVP: precision:  66.10%; recall:  48.49%; FB1:  55.94  292\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  75.99%; recall:  80.57%; FB1:  78.21  6613\n",
      "               PP: precision:  92.48%; recall:  85.62%; FB1:  88.92  2260\n",
      "              PRT: precision:  60.47%; recall:  57.78%; FB1:  59.09  43\n",
      "             SBAR: precision:  79.55%; recall:  44.30%; FB1:  56.91  132\n",
      "               VP: precision:  66.46%; recall:  75.26%; FB1:  70.59  2609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76.57665143331948, 77.47141896435777, 77.02143663031215)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial function for V1,V2 and V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_initial_char_seq(seq, dic):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        temp = torch.zeros([len(string.printable)], dtype=torch.float)\n",
    "        temp[dic[w[0]]] = 1\n",
    "        idxs.append(temp)\n",
    "    result = torch.stack(idxs,0)\n",
    "    return result\n",
    "\n",
    "\n",
    "def prepare_last_char_seq(seq, dic):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        temp = torch.zeros([len(string.printable)], dtype=torch.float)\n",
    "        temp[dic[w[-1]]] = 1\n",
    "        idxs.append(temp)\n",
    "    result = torch.stack(idxs,0)\n",
    "    return result\n",
    "\n",
    "def prepare_mid_char_seq(seq, dic):\n",
    "    idxs = []\n",
    "    for w in seq:\n",
    "        if len(w) >2:\n",
    "            for char in w[1:-1]:\n",
    "                temp = torch.zeros([len(string.printable)], dtype=torch.float)\n",
    "                temp[dic[char]] += 1\n",
    "            idxs.append(temp)\n",
    "        else:\n",
    "            idxs.append(torch.zeros([len(string.printable)], dtype=torch.float))\n",
    "    result = torch.stack(idxs,0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prepare_initial_char_seq function take the first character of each word and set the corresponding index(the position in string.printable()) to one for each first character of the word and then use torch.stack() to stack each v1 for each word.\n",
    "\n",
    "The prepare_last_char_seq function has the same logic as the prepare_initial_char_seq function except it is for the last character of each word.\n",
    "\n",
    "The prepare_mid_char_seq function take the middle character (except first and last character) of each word and set the corresponding index to the count of that character in the word. If the word has no middle character (length <= 2), we use a vector which has 100 zeros to represent it.\n",
    "\n",
    "The result of the functions above will be a 2D tensor of size sentence length times 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified LSTMTaggerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        torch.manual_seed(1)\n",
    "        super(LSTMTaggerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(428, hidden_dim, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence, char):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds_t = embeds.t()\n",
    "        char_t = char.t()\n",
    "        embeds = torch.cat((embeds_t,char_t),0)\n",
    "        embeds = embeds.t()\n",
    "\n",
    "        lstm_out, _ = self.lstm2(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified argmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(self, seq):\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(seq, self.word_to_ix, self.unk)\n",
    "\n",
    "            v1 = prepare_initial_char_seq(seq, self.diction)\n",
    "            v2 = prepare_mid_char_seq(seq, self.diction)\n",
    "            v3 = prepare_last_char_seq(seq, self.diction)\n",
    "            v1 = v1.t()\n",
    "            v2 = v2.t()\n",
    "            v3 = v3.t()\n",
    "            char_in = torch.cat((v1,v2,v3),0)\n",
    "            char_in = char_in.t()\n",
    "\n",
    "            tag_scores = self.model(inputs,char_in)\n",
    "            for i in range(len(inputs)):\n",
    "                output.append(self.ix_to_tag[int(tag_scores[i].argmax(dim=0))])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After concatenate v1, v2 and v3, we get a char_in vector which has size sentence length times 300. The char_in vector can then concatenate with the word embeddings which has size sentence length times 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "        loss_function = nn.NLLLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            for sentence, tags in tqdm.tqdm(self.training_data):\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk)\n",
    "                targets = prepare_sequence(tags, self.tag_to_ix, self.unk)\n",
    "\n",
    "                v1 = prepare_initial_char_seq(sentence, self.diction)\n",
    "                v2 = prepare_mid_char_seq(sentence, self.diction)\n",
    "                v3 = prepare_last_char_seq(sentence, self.diction)\n",
    "                v1 = v1.t()\n",
    "                v2 = v2.t()\n",
    "                v3 = v3.t()\n",
    "\n",
    "                char_in = torch.cat((v1,v2,v3),0)\n",
    "                char_in = char_in.t()\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                tag_scores = self.model(sentence_in,char_in)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch == self.epochs-1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            print(\"saving model file: {}\".format(savefile), file=sys.stderr)\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'unk': self.unk,\n",
    "                        'word_to_ix': self.word_to_ix,\n",
    "                        'tag_to_ix': self.tag_to_ix,\n",
    "                        'ix_to_tag': self.ix_to_tag,\n",
    "                    }, savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "The method we are using is referenced from the option one of combine the semi-Character RNN with phrasal chunker in the homework page. According to the baseline description, we tried to seperate each word into three vectors. V1 stores the first character, V3 stores the last character and V2 stores the remaining characters. When we created these three vectors, we use torch.stack() to stack each word vector in order to get the result. \n",
    "    \n",
    "We transported three vectors which represented word fractions for every sentence then used torch.cat() to concatenate them to get a 2D tensor of size 300 times sentence length. After that, we transported that 2D tensor in order to get a new 2D tensor of size sentence length times 300. \n",
    "\n",
    "In class LSTMTaggerModel, we replaced \"self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=False)\" by \"self.lstm2 = nn.LSTM(428, hidden_dim, bidirectional=False)\" because len(string.printable) was 100 and we concatenated 3 such vectors which would change the input from 128 to 428. \n",
    "\n",
    "We used the same way to concatenate the sentence length 2D tensor of character vectors with the word embeddings which was a 2D tensor of size sentence length times 128. Eventually, we got Score(dev) = 77.0214.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
