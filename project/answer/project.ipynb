{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as py\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tree import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "The problem our group choose is Mining and Summarizing Customer Review. The reason why we choose it is because as the rapid development of Internet, more and more products are sold online, at the same time, people are willing to shop online. Hence, products' reviews become much more important for both customers and manufactures. For potential customers, it can help them to make decision whether to buy it or not. At the same time, it is necessary for manufactures to improve their products' quality according to the reviews.But since part of the reviews are non-sense, we aim to fetch useful sentence from each review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "The algorithms and build-in functions we used sre NLTK tagging and NLTK tree for finding\n",
    "noun phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK token for tagging each word in sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(raw):\n",
    "    sentlist=[]\n",
    "    sents=nltk.sent_tokenize(raw)\n",
    "    for sent in sents:\n",
    "        tokenw=nltk.word_tokenize(sent)\n",
    "        tagw=nltk.pos_tag(tokenw)\n",
    "        sentlist.append(tagw)\n",
    "    return sentlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK tree for finding noun-phrases in the sentence we tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinionS(sent):\n",
    "    ow=False\n",
    "    adj=False\n",
    "    for word in sent:\n",
    "        if word[1]=='NN' or word[1]=='NNS' or word[1]=='NNP' or word[1]=='NNPS':\n",
    "            ow=True\n",
    "        if word[1]=='JJ' or word[1]=='JJR'or word[1]=='JJS':\n",
    "            adj=True\n",
    "    return ow*adj\n",
    "\n",
    "def NounPhrase(sent):\n",
    "    feature=[]\n",
    "    grammer = r\"\"\"\n",
    "        NP:\n",
    "            {<NN|NNS><NN|NNS><NN|NNS>}\n",
    "            {<NN|NNS><NN|NNS>}\n",
    "            {<NN|NNS><IN><NN|NNS><NN|NNS>}\n",
    "            {<NN|NNS><IN><NN|NNS>}\n",
    "            {<NN|NNS>}\n",
    "    \"\"\"\n",
    "    cp=nltk.RegexpParser(grammer)\n",
    "    result = cp.parse(sent)\n",
    "    for subtree in result.subtrees(filter = lambda t:t.label() == 'NP'):\n",
    "        feature.append (subtree.leaves())\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We conduct our experiments by randomly picking five electronics products which are 2 digital cameras, 1 DVD player, 1 MP3 player, 1 cellular phone and their customer reviews from Amazon.com and C|net.com which are provided by the paper[1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All of our code is written by ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting stop words for making sure these words are not in noun-phrases like 'your','haven','themselves' and so on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input reading file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(addr):\n",
    "    f=open(addr,'r')\n",
    "    raw=f.read()\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK token for tagging each word in sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(raw):\n",
    "    sentlist=[]\n",
    "    sents=nltk.sent_tokenize(raw)\n",
    "    for sent in sents:\n",
    "        tokenw=nltk.word_tokenize(sent)\n",
    "        tagw=nltk.pos_tag(tokenw)\n",
    "        sentlist.append(tagw)\n",
    "    return sentlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK tree for finding noun-phrases in the sentence we tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinionS(sent):\n",
    "    ow=False\n",
    "    adj=False\n",
    "    for word in sent:\n",
    "        if word[1]=='NN' or word[1]=='NNS' or word[1]=='NNP' or word[1]=='NNPS':\n",
    "            ow=True\n",
    "        if word[1]=='JJ' or word[1]=='JJR'or word[1]=='JJS':\n",
    "            adj=True\n",
    "    return ow*adj\n",
    "\n",
    "def NounPhrase(sent):\n",
    "    feature=[]\n",
    "    grammer = r\"\"\"\n",
    "        NP:\n",
    "            {<NN|NNS><NN|NNS><NN|NNS>}\n",
    "            {<NN|NNS><NN|NNS>}\n",
    "            {<NN|NNS><IN><NN|NNS><NN|NNS>}\n",
    "            {<NN|NNS><IN><NN|NNS>}\n",
    "            {<NN|NNS>}\n",
    "    \"\"\"\n",
    "    cp=nltk.RegexpParser(grammer)\n",
    "    result = cp.parse(sent)\n",
    "    for subtree in result.subtrees(filter = lambda t:t.label() == 'NP'):\n",
    "        feature.append (subtree.leaves())\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup positive and negative list and store them in seed list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive=['good','pretty','fantastic','cool','nice','amazing','excellent','perfect','outstanding','clear','remarkable','gorgeous','wonderful','awesome','upbeat','favorable','cheerful','pleased','appealing']\n",
    "negative=['bad','disappointing','dull','ugly','terrible','disgraceful','poor','shoddy','awful','noisome','disgusting','frustrating','awkward','irritating','weired']\n",
    "seed_list = {}\n",
    "for word in positive:\n",
    "    seed_list[word] = 'positive'\n",
    "for word in negative:\n",
    "    seed_list[word] = 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup negation word list for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_word = [\"no\",\"not\",\"yet\",\"never\",\"hardly\",\"little\",\"few\",\"none\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup UI when run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Apex AD2600 Progressive-scan DVD player cleaned.txt\n",
      "\n",
      "(2) Canon G3 cleaned.txt\n",
      "\n",
      "(3) Creative Labs Nomad Jukebox Zen Xtra 40GB cleaned.txt\n",
      "\n",
      "(4) Nikon coolpix 4300 cleaned.txt\n",
      "\n",
      "(5) Nokia 6610 cleaned.txt\n",
      "\n",
      "Enter file number you wish to process: 2\n",
      "Star calculating results...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"(1) Apex AD2600 Progressive-scan DVD player cleaned.txt\\n\")\n",
    "print(\"(2) Canon G3 cleaned.txt\\n\")\n",
    "print(\"(3) Creative Labs Nomad Jukebox Zen Xtra 40GB cleaned.txt\\n\")\n",
    "print(\"(4) Nikon coolpix 4300 cleaned.txt\\n\")\n",
    "print(\"(5) Nokia 6610 cleaned.txt\\n\")\n",
    "\n",
    "val = input(\"Enter file number you wish to process: \")\n",
    "\n",
    "\n",
    "if val == '1':\n",
    "    file_name = 'Apex AD2600 Progressive-scan DVD player cleaned.txt'\n",
    "elif val == '2':\n",
    "    file_name = 'Canon G3 cleaned.txt'\n",
    "elif val == '3':\n",
    "    file_name = 'Creative Labs Nomad Jukebox Zen Xtra 40GB cleaned.txt'\n",
    "elif val == '4':\n",
    "    file_name = 'Nikon coolpix 4300 cleaned.txt'\t\n",
    "elif val == '5':\n",
    "    file_name = 'Nokia 6610 cleaned.txt'\n",
    "else:\n",
    "    raise Exception('input should be 1-5. The value of input was: {}'.format(val))\n",
    "\n",
    "file = 'data/' + file_name\n",
    "print(\"Star calculating results...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching feature in the opinion sentence and store them in candidate list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=readfile(file)\n",
    "tokenized=token(raw)\n",
    "OS=[sent for sent in tokenized if opinionS(sent)]\n",
    "opinionS_N=len(OS)\n",
    "nounphrase=[]\n",
    "for sent in OS:\n",
    "    nounphrase.append(NounPhrase(sent))\n",
    "nounphrase_N=len(nounphrase)\n",
    "candidate=[]\n",
    "for i in range(0,nounphrase_N):\n",
    "    for j in range(0,len(nounphrase[i])):\n",
    "        f=''\n",
    "        for x in range(0,len(nounphrase[i][j])):\n",
    "            if (nounphrase[i][j][x][0] not in stop_words or nounphrase[i][j][x][0] in ['of','for']) and (x!=len(nounphrase[i][j])-1):\n",
    "                f+=nounphrase[i][j][x][0]+' '\n",
    "            elif(nounphrase[i][j][x][0] not in stop_words or nounphrase[i][j][x][0] in ['of','for']) and (x==len(nounphrase[i][j])-1):\n",
    "                f+=nounphrase[i][j][x][0]\n",
    "        candidate.append(f)\n",
    "candidate=[elem for elem in candidate if elem.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding features that appears > 2% in candidate list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateDic={}\n",
    "for i in candidate:\n",
    "    if i not in candidateDic:\n",
    "        candidateDic[i]=1\n",
    "    else:\n",
    "        candidateDic[i]+=1\n",
    "\n",
    "features=[elem for elem in candidateDic.keys() if candidateDic[elem]/opinionS_N > 0.02]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary to store the sentences that features appeared (key = feature, context = sentence index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresNS=[]\n",
    "for f in features:\n",
    "    s=f\n",
    "    s=s.replace(' ','')\n",
    "    featuresNS.append(s)\n",
    "\n",
    "featuresDic={}\n",
    "\n",
    "for i in range(0,len(OS)):\n",
    "    sentNS=''\n",
    "    sent_N=len(OS[i])\n",
    "    for j in range(0,sent_N):\n",
    "        sentNS+=OS[i][j][0]\n",
    "        \n",
    "    for z in range(0,len(featuresNS)):\n",
    "        if featuresNS[z] in sentNS:\n",
    "            if features[z] not in featuresDic:\n",
    "                featuresDic[features[z]]=[i]\n",
    "            else:\n",
    "                featuresDic[features[z]].append(i)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing opinion sentences' tag for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag(OS):\n",
    "    output = []\n",
    "    for sent in OS:\n",
    "        new_sent = []\n",
    "        for word in sent:\n",
    "            new_sent.append(word[0])\n",
    "        output.append(new_sent)\n",
    "    return output\n",
    "OS_notag = remove_tag(OS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get the adjective word in opinion sentence. We first check whether the adjective word is in the seed_list. If the word is in seed_list, we do nothing. If the word's synonyms is already in the seed_list, we give the adjective word same orientation as it's synonyms. Otherwise, we check whether it's antonyms is in seed_list. If the antonyms is in the seed_list, we give the adjective words opposite orientation as antonyms' orientation. If the words has neither synonyms nor antonyms in the seed_list, we discard this adjective word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_syn_ant(word):\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "    for syn in wn.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            if l.antonyms():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "    return synonyms, antonyms\n",
    "\n",
    "def negation(orientation):\n",
    "    if orientation == \"positive\":\n",
    "        orientation = \"negative\"\n",
    "    else:\n",
    "        orientation = \"positive\"\n",
    "    return orientation\n",
    "\n",
    "def OrientationPrediction(adj_list, seed_list):\n",
    "    while True:\n",
    "        size1 = len(seed_list)\n",
    "        adj_list, seed_list = OrientationSearch(adj_list, seed_list)\n",
    "        size2 = len(seed_list)\n",
    "        if size1 == size2:\n",
    "            break\n",
    "\n",
    "    return adj_list, seed_list\n",
    "\n",
    "def OrientationSearch(adj_list, seed_list):\n",
    "    added = False\n",
    "    for adj in adj_list:\n",
    "        adj_syn, adj_ant = find_syn_ant(adj)\n",
    "        for syn in adj_syn:\n",
    "            if syn in seed_list:\n",
    "                adj_orientation = seed_list[syn]\n",
    "                seed_list[adj] = adj_orientation\n",
    "                added = True\n",
    "                break\n",
    "        if added == False:\n",
    "            for ant in adj_ant:\n",
    "                if ant in seed_list:\n",
    "                    adj_orientation = negation(seed_list[ant])\n",
    "                    seed_list[adj] = adj_orientation\n",
    "                    added = True\n",
    "                    break\n",
    "    return adj_list, seed_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding words before and after feature words in opinion sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_word(word, sentence, size):\n",
    "    word_pos = sentence.index(word)\n",
    "    if len(sentence) <= size:\n",
    "        window = sentence\n",
    "    elif word_pos < size:\n",
    "        window = sentence[0:word_pos + size]\n",
    "    elif len(sentence) - word_pos < size:\n",
    "        window = sentence[word_pos - size:-1]\n",
    "    else:\n",
    "        window = sentence[word_pos - size: word_pos + size]\n",
    "\n",
    "    return window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update seed_list by each adjective words in each opinion sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in featuresDic:\n",
    "    for sentence_index in featuresDic[feature]:\n",
    "        sentence = OS_notag[sentence_index]\n",
    "        if feature in sentence:\n",
    "\n",
    "            window = sentence\n",
    "            adjs = []\n",
    "            window_tag = nltk.pos_tag(window)\n",
    "            for word_tag in window_tag:\n",
    "                if word_tag[1] == 'JJ':\n",
    "                    adjs.append(word_tag[0])\n",
    "\n",
    "            adjs, seed_list = OrientationPrediction(adjs,seed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to identify specific adjective word's orientation in specific sentence. Return 1 if it is positive. Otherwise, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordOrientation(word, sentence):\n",
    "    orientation = seed_list[word]\n",
    "    window = close_word(word, sentence, 5)\n",
    "    for neg_word in negation_word:\n",
    "        if neg_word in window:\n",
    "            orientation = negation(orientation)\n",
    "\n",
    "    if orientation == \"positive\":\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the sentence by it's orientation which is calculated by the adjective words in it.\n",
    "<br>&emsp;&emsp;1.If adjective word orientation is postive, sentence orientation plus 1. \n",
    "<br>&emsp;&emsp;2.If the adjective word orientation is negative, the sentence orientation minus 1.\n",
    "<br>After calculating the adjective words, target sentence is:\n",
    "    <br>&emsp;&emsp;1.positive if the sentence orientation > 0\n",
    "    <br>&emsp;&emsp;2.negative if the sentence orientation < 0\n",
    "<br>When target sentence orientation = 0, we continue calculating it's orientation according to the effective adjective word which we set 5 words before and after feature.\n",
    "    <br>&emsp;&emsp;1.If effective adjective word is positive, sentence orientation plus 1\n",
    "    <br>&emsp;&emsp;2.If effective adjective word is negative, sentence orientation minus 1\n",
    "<br>After calculating the effective adjective words, target sentence is:\n",
    "    <br>&emsp;&emsp;1.positive if the sentence orientation > 0\n",
    "    <br>&emsp;&emsp;2.negative if the sentence orientation < 0\n",
    "    <br>&emsp;&emsp;3.neutral if the sentence orientation = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceOrientation = {}\n",
    "sentence_effective = {}\n",
    "sentence_opw = {}\n",
    "sentence_feature = {}\n",
    "\n",
    "for i,sentence in enumerate(OS_notag):\n",
    "    orientation = 0\n",
    "    sentence_opw[i] = []\n",
    "    sentence_effective[i] = []\n",
    "    sentence_feature[i] = []\n",
    "\n",
    "    for feature in featuresDic:\n",
    "        if feature in sentence:\n",
    "            sentence_feature[i].append(feature)\n",
    "\n",
    "            eff_window = close_word(feature, sentence, 5)\n",
    "            eff_tag = nltk.pos_tag(eff_window)\n",
    "            for tag in eff_tag:\n",
    "                if tag[1] == 'JJ' and tag[0] not in sentence_effective[i]:\n",
    "                    sentence_effective[i].append(tag[0])\n",
    "\n",
    "    for word in sentence:\n",
    "        if word in seed_list:\n",
    "            sentence_opw[i].append(word)\n",
    "\n",
    "    for op in sentence_opw[i]:\n",
    "        if op in seed_list:\n",
    "            orientation += wordOrientation(op,sentence)\n",
    "\n",
    "    if orientation > 0:\n",
    "        sentenceOrientation[i] = \"Positive\"\n",
    "    elif orientation < 0:\n",
    "        sentenceOrientation[i] = \"Negative\"\n",
    "    else:\n",
    "        for eff_op in sentence_effective[i]:\n",
    "            if eff_op in seed_list:\n",
    "                orientation += wordOrientation(eff_op,sentence)\n",
    "        if orientation > 0:\n",
    "            sentenceOrientation[i] = \"Positive\"\n",
    "        elif orientation < 0:\n",
    "            sentenceOrientation[i] = \"Negative\"\n",
    "        else:\n",
    "            sentenceOrientation[i] = \"Neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create featureOrientation for output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureOrientation = {}\n",
    "\n",
    "for feature in featuresDic:\n",
    "    featureOrientation[feature] = {\"positive\":[], \"negative\":[], \"neutral\":[]}\n",
    "\n",
    "    for sentence_index in featuresDic[feature]:\n",
    "\n",
    "        if sentenceOrientation[sentence_index] == \"Positive\" and sentence_index not in featureOrientation[feature][\"positive\"]:\n",
    "            featureOrientation[feature][\"positive\"].append(sentence_index)\n",
    "\n",
    "        elif sentenceOrientation[sentence_index] == \"Negative\" and sentence_index not in featureOrientation[feature][\"negative\"]:\n",
    "            featureOrientation[feature][\"negative\"].append(sentence_index)\n",
    "\n",
    "        elif sentenceOrientation[sentence_index] == \"Neutral\" and sentence_index not in featureOrientation[feature][\"neutral\"]:\n",
    "            featureOrientation[feature][\"neutral\"].append(sentence_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two feature dictionaries in featureOrientation if those two features have high similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = {\"positive\":[], \"negative\":[], \"neutral\":[]}\n",
    "    for key in z.keys():\n",
    "        z[key] = x[key] + y[key]\n",
    "    return z\n",
    "\n",
    "duplicate_feature = []\n",
    "for i,prev_feature in enumerate(features):\n",
    "    for feature in features[i+1:]:\n",
    "        s = SequenceMatcher(None, prev_feature, feature)\n",
    "        if s.ratio() > 0.7 and s.ratio() != 1.0:\n",
    "            featureOrientation[feature] = merge_two_dicts(featureOrientation[prev_feature],featureOrientation[feature])\n",
    "            duplicate_feature.append(prev_feature)\n",
    "\n",
    "for feature in duplicate_feature:\n",
    "    del featureOrientation[feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the result orientation to file in output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start outputing results to output/Canon G3_output.txt\n",
      "\n",
      "Output completes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def list_sentence(input):\n",
    "    return [[' '.join(i)] for i in input]\n",
    "\n",
    "sentences = list_sentence(OS_notag)\n",
    "\n",
    "output_file = 'output/' + file_name.replace(' cleaned','_output')\n",
    "print(\"Start outputing results to \" + output_file + '\\n')\n",
    "\n",
    "output = open(output_file,'w')\n",
    "# print output loop\n",
    "for feature in featureOrientation:\n",
    "    if featureOrientation[feature][\"positive\"] != [] and featureOrientation[feature][\"negative\"] != []:\n",
    "        output.write(feature + '\\n')\n",
    "        if featureOrientation[feature][\"positive\"] != []:\n",
    "            output.write(\"Positive:\" + '\\n')\n",
    "            for index in featureOrientation[feature][\"positive\"]:\n",
    "                output.write(sentences[index][0].replace(\"#\", \"\").strip(\" \") + '\\n')\n",
    "\n",
    "        if featureOrientation[feature][\"negative\"] != []:\n",
    "            output.write(\"Negative:\"+ '\\n')\n",
    "            for index in featureOrientation[feature][\"negative\"]:\n",
    "                output.write(sentences[index][0].replace(\"#\", \"\").strip(\" \") + '\\n')\n",
    "\n",
    "        if featureOrientation[feature][\"neutral\"] != []:\n",
    "            output.write(\"neutral:\"+ '\\n')\n",
    "            for index in featureOrientation[feature][\"neutral\"]:\n",
    "                output.write(sentences[index][0].replace(\"#\", \"\").strip(\" \") + '\\n')\n",
    "\n",
    "output.close()\n",
    "\n",
    "print(\"Output completes\" + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation, we use the tagged file(manually tagged by the paper's contributors[1]) given by the paper[1] to test the accuracy of our outcoming results. The sentence orientation accuracy = the number of orientation sentence matched with tagged file/ total opinion sentence. The sentence extraction accuracy = the number of opinion sentence matched with tagged file/total opinion sentence. Then, we compare the precision we obtain with the precision given in the paper[1] to conclude the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to evaluate the output?(y/n): y\n",
      "Start evaluating output results ...\n",
      "\n",
      "Sentence orientation accuracy is:\n",
      "0.626\n",
      "\n",
      "\n",
      "Opinion sentence extraction precision is:\n",
      "0.778\n",
      "\n",
      "\n",
      "Program finished\n"
     ]
    }
   ],
   "source": [
    "val = input(\"Do you want to evaluate the output?(y/n): \")\n",
    "\n",
    "if val.lower() == 'y':\n",
    "    print(\"Start evaluating output results ...\" + '\\n')\n",
    "    # evaluation\n",
    "    eval_list = []\n",
    "    for feature in featureOrientation:\n",
    "\n",
    "        for sentence_index in featureOrientation[feature][\"positive\"]:\n",
    "            if sentence_index not in eval_list:\n",
    "                eval_list.append(sentence_index)\n",
    "\n",
    "        for sentence_index in featureOrientation[feature][\"negative\"]:\n",
    "            if sentence_index not in eval_list:\n",
    "                eval_list.append(sentence_index)\n",
    "\n",
    "        for sentence_index in featureOrientation[feature][\"neutral\"]:\n",
    "            if sentence_index not in eval_list:\n",
    "                eval_list.append(sentence_index)\n",
    "\n",
    "\n",
    "    eval_list.sort()\n",
    "\n",
    "\n",
    "    addr = file.replace(' cleaned','')\n",
    "    f = open(addr,'r')\n",
    "    raw = f.read()\n",
    "    add = False\n",
    "\n",
    "    in_sentence = ''\n",
    "    for i,char in enumerate(raw):\n",
    "        if char == '#' and raw[i - 1] == ']':\n",
    "            add = True\n",
    "        if add:\n",
    "            in_sentence += raw[i]\n",
    "        if raw[i] == '.' or raw[i] == '!':\n",
    "            add = False\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    exist_eval_list = in_sentence.split('##')\n",
    "    exist_eval_list.remove('')\n",
    "\n",
    "    total_op = len(exist_eval_list)\n",
    "    total_correct = 0\n",
    "\n",
    "    for index in eval_list:\n",
    "        output_s = sentences[index][0].replace(\"#\", \"\").strip(\" \")\n",
    "        for sentence in exist_eval_list:\n",
    "            s = SequenceMatcher(None, output_s, sentence)\n",
    "            if s.ratio() > 0.9:\n",
    "                total_correct += 1\n",
    "                break\n",
    "\n",
    "    print(\"Sentence orientation accuracy is:\")\n",
    "    print(\"%.3f\" % (total_correct / total_op))\n",
    "    print('\\n')\n",
    "\n",
    "    total_correct = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence[0].replace(\"#\", \"\").strip(\" \")\n",
    "        for sentence_comp in exist_eval_list:\n",
    "            s = SequenceMatcher(None, sentence, sentence_comp)\n",
    "            if s.ratio() > 0.9:\n",
    "                total_correct += 1\n",
    "                break\n",
    "\n",
    "\n",
    "    print(\"Opinion sentence extraction precision is:\")\n",
    "    print(\"%.3f\" % (total_correct / total_op))\n",
    "    print('\\n')\n",
    "    \n",
    "    print(\"Program finished\")\n",
    "\n",
    "elif val.lower() == 'n':\n",
    "    print(\"Program finished\")\n",
    "\n",
    "else:\n",
    "    raise Exception('input should be y/n. The value of input was: {}'.format(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OS = opinion sentence\n",
    "<br>SO = sentence orientation\n",
    "<br>original = result from paper's model\n",
    "\n",
    "|  Product|OS extraction(original)| OS extraction | SO accuracy(original）|SO accuracy |\n",
    "| --- |---|---|---|---|\n",
    "|  Digital camera1  |0.643|0.739|0.927|0.925|\n",
    "|  Digital camera2  |0.554|0.778|0.946|0.626|\n",
    "|  Cellular phone  |0.815|0.709|0.764|0.506|\n",
    "|   Mp3 player  |0.589|0.825|0.842|0.706|\n",
    "|  DVD plyer   |0.607|0.770|0.730|0.589|\n",
    "|  Average    |0.642|0.764|0.842|0.591|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improve the opinion sentence extraction precision by effectively extracting more opinion sentences from data files according to adjective and noun phrase existence. Therefore, the hitting rate of our algorithm is higher. Sentence orientation accuracy is lower because we have more features in each comment so there are more error in our algorithm. The place we use pruning and the method of pruning is different from the algorithm in paper which also causes some differences in the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to improve our algorithm and increase the accuracy by implementing better feature pruning algorithm. Then we need to deal with sentence that contains implicit features, such as \"It cannot fit in my pockets\" which is talking about size but without word 'size'. Finally, we will try to use machine learning in future in order to figure out how to use verbs and nouns in opinion sentences for purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Minqing Hu and Bing Liu. Mining and summarizing customer reviews. 13:168–177, August2004."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
