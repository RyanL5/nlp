{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hw1 Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo code of iterative segmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    while heap is nonempty:\n",
    "        entry = top entry in the heap\n",
    "\n",
    "        if chart[endindex] has a previous entry, preventry\n",
    "            if entry has a higher probability than preventry:\n",
    "                chart[endindex] = entry\n",
    "            if entry has a lower or equal probability than preventry:\n",
    "                continue\n",
    "        else\n",
    "            chart[endindex] = entry\n",
    "\n",
    "        for each newword that matches input starting at position endindex+1\n",
    "            newentry = Entry(newword, endindex+1, entry.log-probability + logPw(newword), entry)\n",
    "            if newentry does not exist in heap:\n",
    "                insert newentry into heap\n",
    "\n",
    "    finalindex is the length of input\n",
    "    finalentry = chart[finalindex]\n",
    "    The best segmentation starts from finalentry and follows the back-pointer recursively until the first word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual implementation of iterative segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "\n",
    "        ## Initialize the heap ##  \n",
    "        heap = []\n",
    "        splitted_text = self.splits(text)\n",
    "\n",
    "        for w,rem in splitted_text:\n",
    "            #  Entry(word, start-position, log-probability, back-pointer)\n",
    "            entry = (w, 0, math.log10(self.Pw(w)), None)\n",
    "            heap.append(entry)\n",
    "\n",
    "        heap.sort(key=takeProb)\n",
    "\n",
    "        # # initialize chart: the dynamic programming table to store the argmax for every prefix of input\n",
    "        # # indexed by character position in input\n",
    "        chart = [None for i in range(len(text))]\n",
    "\n",
    "        ## Iteratively fill in chart[i] for all i ##\n",
    "        while heap:\n",
    "            entry = heap.pop()\n",
    "            entry_word = entry[0]\n",
    "            entry_start = entry[1]\n",
    "            entry_prob = entry[2]\n",
    "            entry_backptr = entry[3]\n",
    "\n",
    "            # get the endindex based on the length of the word in entry\n",
    "            endindex = entry_start + len(entry_word)  - 1 \n",
    "\n",
    "            if chart[endindex]:\n",
    "                preventry = chart[endindex]\n",
    "                preventry_word = preventry[0]\n",
    "                preventry_start = preventry[1]\n",
    "                preventry_prob = preventry[2]\n",
    "                preventry_backptr = preventry[3]\n",
    "\n",
    "                if entry_prob > preventry_prob:\n",
    "                    # if entry has a higher probability than preventry\n",
    "                    chart[endindex] = entry\n",
    "                if entry_prob <= preventry_prob:\n",
    "                     ## we have already found a good segmentation until endindex ##\n",
    "                    continue\n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "            \n",
    "            splitted_text = self.splits(text[(endindex + 1):])\n",
    "\n",
    "            for newword,rem in splitted_text:\n",
    "                newentry = (newword, endindex + 1, entry_prob + math.log10(self.Pw(newword)), entry)\n",
    "                if newentry not in heap:\n",
    "                    heap.append(newentry)\n",
    "\n",
    "            heap.sort(key=takeProb)\n",
    "\n",
    "        # ## Get the best segmentation ##\n",
    "        finalindex = len(text)\n",
    "        finalentry = chart[finalindex - 1]\n",
    "        segmentation = self.recursive_back(finalentry)\n",
    "\n",
    "        return segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the unigram solution on dev following the iterative segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议 \n",
      "新华社 上海 八月 三十一日 电 （ 记者 白国良 、 夏儒阁 ） \n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。 \n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"data/count_1w.txt\"),N = None, missingfn = smooth)\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = segmenter.segment(line.strip())\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add smooth function to give probability for unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(word, N):\n",
    "    return 10000./(N * 1000**len(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual implementation of iterative segmenter for bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(self, text, prev = '<S>'):\n",
    "        if (text, prev) in self.table:\n",
    "            return self.table[(text, prev)]\n",
    "\n",
    "        if not text: return 0.0, []\n",
    "\n",
    "        segmentation = []\n",
    "        \n",
    "        splitted_text = self.splits(text)\n",
    "\n",
    "        for first, rem in splitted_text:\n",
    "\n",
    "            # recursively call segment\n",
    "            restProb, rest = self.segment(rem,first)\n",
    "\n",
    "            prob = log10(self.cPw(prev,first))\n",
    "            segmentation.append((prob + restProb, [first] + rest))\n",
    "\n",
    "        finalentry = segmentation[0]\n",
    "        for index in range(len(segmentation)):\n",
    "            if segmentation[index][0] > finalentry[0]:\n",
    "                finalentry = segmentation[index]\n",
    "\n",
    "        self.table[text, prev] = finalentry\n",
    "\n",
    "        return finalentry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the iterative segmenter output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.93\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "We tried to use unigram and bigram. The algorithm of unigrame is iterative segmenter which is similar to the algorithm on homework page. We designed another alogprithm for bigram which made the result better. A variable L in function splits is used to limit the maximum length of a word. According to our tests, we chose 5 to be the maximun length.  We also implemented a smooth function to give unknown words which does not appeared in corpus a probability of 10000./(N * 1000**len(word)) in order to avoid zero probability. Each unknown words will be punished harder compared to previous homework since Chinese words are characters which do not have long length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
